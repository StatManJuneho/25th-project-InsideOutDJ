# -*- coding: utf-8 -*-
"""model.ipynb의 사본

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mdOh-cU76M516zD6npv81Fs5x0Nh3Ozj
"""


import torch
from transformers import BertTokenizer, BertForSequenceClassification
import pandas as pd
import nltk
from keybert import KeyBERT
from sentence_transformers import SentenceTransformer
from nltk.corpus import stopwords
import json
import os

# 토크나이저 로드
tokenizer = BertTokenizer.from_pretrained('monologg/kobert')
# 모델 초기화
model = BertForSequenceClassification.from_pretrained('monologg/kobert', num_labels=5)
# 모델의 가중치 로드 (CPU로 매핑)
loaded_data = torch.load('./kobert_emotion_model.pth', map_location=torch.device('cpu'))
model.load_state_dict(loaded_data['model_state_dict'], strict=False)

device = torch.device('cpu')
model.to(device)
model.eval()

"""
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
model.eval()
"""

import kss
import nltk

nltk.download('punkt')

def split_sentences(paragraph):
    sentences = kss.split_sentences(paragraph)
    return sentences

# 레이블을 좌표로 매핑
label_to_emotion_vector = {
    0: (1, 1),
    1: (1, -1),
    2: (-1, -1),
    3: (-1, 1),
    4: (0, 0)
}

# 예측 함수
def predict_emotion(model, tokenizer, sentence):
    inputs = tokenizer.encode_plus(
        sentence,
        add_special_tokens=True,
        max_length=64,
        return_token_type_ids=False,
        padding='max_length',
        truncation=True,
        return_attention_mask=True,
        return_tensors='pt',
    )
    input_ids = inputs['input_ids'].to(device)
    attention_mask = inputs['attention_mask'].to(device)

    with torch.no_grad():
        outputs = model(input_ids, attention_mask=attention_mask)
        logits = outputs.logits

    predicted_class = torch.argmax(logits, dim=1).cpu().numpy()[0]
    emotion_vector = label_to_emotion_vector[predicted_class]

    return emotion_vector

# 문단의 전체 감정을 계산하고 정규화하는 함수
def calculate_paragraph_emotion(paragraph):
    sentences = split_sentences(paragraph)
    total_x, total_y = 0, 0

    for sentence in sentences:
        emotion_vector = predict_emotion(model, tokenizer, sentence)
        total_x += emotion_vector[0]
        total_y += emotion_vector[1]
        print(f"Sentence: {sentence}")
        print(f"Emotion Vector: {emotion_vector}")
        print("-" * 50)

    # 문단 전체의 감정을 문장 수로 나누어 정규화
    normalized_emotion = (total_x / len(sentences), total_y / len(sentences))
    print(f"Normalized Paragraph Emotion: {normalized_emotion}")

    return normalized_emotion

# 예시 문단
paragraph = "오늘 하루는 정말 힘들었어. 아침부터 지각할 뻔했고, 일이 산더미처럼 쌓여있었어. 사람들과의 대화도 어색하고, 모든 게 잘 안 풀리는 느낌이었어. 점심시간에는 혼자 밥을 먹었는데, 외롭다는 생각이 문득 들었어. 집에 와서도 기운이 없고, 그냥 아무것도 하고 싶지 않아. 이런 날도 있겠지만, 정말 지치고 답답해. 내일은 조금 더 나아지길 바라며, 오늘은 일찍 잠들어야겠어."

# 문단의 감정을 계산하고 정규화
normalized_emotion = calculate_paragraph_emotion(paragraph)

import pandas as pd
import numpy as np

# 업로드된 CSV 파일 불러오기
file_path = './tracks_final.csv'
df = pd.read_csv(file_path)

def get_quadrant(x, y):
    if x >= 0 and y >= 0:
        return 1
    elif x < 0 and y >= 0:
        return 2
    elif x <= 0 and y < 0:
        return 3
    else:
        return 4

def calculate_distance(x, y):
    return np.sqrt(x**2 + y**2)

def filter_by_intensity(df, distance):
    if distance <= 0.1:
        intensity_label = 'neutral'
    elif 0.1 < distance <= 0.4:
        intensity_label = 'low'
    elif 0.4 < distance <= 0.7:
        intensity_label = 'medium'
    else:
        intensity_label = 'high'

    return df[df['intensity'] == intensity_label]

def get_songs_by_emotion_and_intensity(df, normalized_emotion):
    # 사분면 판별
    quadrant = get_quadrant(*normalized_emotion)

    # 사분면에 맞는 노래 데이터 필터링
    filtered_df = df[df['emotion'] == quadrant]

    # (0, 0)에서 normalized_emotion까지의 거리 계산
    distance = calculate_distance(*normalized_emotion)

    # 강도에 따른 추가 필터링
    final_songs = filter_by_intensity(filtered_df, distance)

    return final_songs

matching_songs = get_songs_by_emotion_and_intensity(df, normalized_emotion)

# 결과 확인
print(matching_songs)

import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import ast

# KeyBERT 모델 로드
kw_model = KeyBERT()

# 다국어 SBERT 모델 로드
sbert_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

# 키워드 추출 함수 정의
def extract_keywords(text):
    keywords = kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 2), stop_words='english', top_n=5)
    return [kw[0] for kw in keywords]  # 키워드만 추출

# 키워드 임베딩 함수 정의
def embed_keywords(keywords):
    embeddings = sbert_model.encode(keywords)
    return embeddings.tolist()  # 리스트 형식으로 변환하여 저장

# 'keyword_embedding' 칼럼을 리스트로 변환
matching_songs['keyword_embedding'] = matching_songs['keyword_embedding'].apply(ast.literal_eval)

# 입력 텍스트에 대해 키워드 추출 및 임베딩
input_text = paragraph  # 감정 분석했던 문단 텍스트를 사용
input_keywords = extract_keywords(input_text)
input_embedding = embed_keywords(input_keywords)

# 유사도 계산 함수
def calculate_similarity(input_embedding, song_embedding):
    # numpy 배열로 변환 (2D 형태)
    input_embedding = np.array(input_embedding).reshape(1, -1)  # (1, 384)
    song_embedding = np.array(song_embedding).reshape(1, -1)  # (1, 384)

    # 코사인 유사도 계산
    return cosine_similarity(input_embedding, song_embedding)[0][0]

# 노래의 keyword_embedding 컬럼에서 임베딩 값을 가져오고 유사도 계산
matching_songs['similarity'] = matching_songs['keyword_embedding'].apply(lambda x: calculate_similarity(input_embedding, x))

# 유사도 상위 5개 노래 선택
top_5_songs = matching_songs.sort_values(by='similarity', ascending=False).head(5)

print(top_5_songs[['track_name', 'artist_name']])

# 결과를 지정된 디렉토리에 JSON 파일로 저장
output_path = r'C:\Users\gu051\1workspace1\신기플\top_5_songs.json'
top_5_songs_json = top_5_songs[['track_name', 'artist_name']].to_dict(orient='records')
with open(output_path, 'w', encoding='utf-8') as json_file:
    json.dump(top_5_songs_json, json_file, ensure_ascii=False, indent=4)


